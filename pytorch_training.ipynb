{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine Tuning a PyTorch Model on Our Fruits Dataset\n",
    "This notebook will demonstrate how to use PyTorch to fine tune a pre-existing object detection model for our fruits data set.\n",
    "\n",
    "We will start by loading a pre-trained Faster RCNN model using the PyTorch library and modify it so that it can classify the various fruits in our data set.\n",
    "\n",
    "Afterwards, we will write our training, evaluation, and testing loops used for training our model.\n",
    "Our custom Dataset class will be used in combination with a DataLoader for batching our data for training.\n",
    "During training, we will set it up so that if training is interrupted it can be resumed from where it stopped.\n",
    "We will see how to save our \"best\" model found during our training loop.\n",
    "Once training is completed, we will use our test data set to evaluate the precision, recall, and accuracy of our model.\n",
    "\n",
    "Finally, we will discuss next steps, including making our model work with the PyTorch lightning framework."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the Faster-RCNN Pre-Trained Model\n",
    "We'll load our model from the TorchVision module. We will specify a cache directory that tells PyTorch where to download our model to."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Specify a cache_dir we can use to tell PyTorch where to place the downloaded models\n",
    "cache_dir = './data/models/'\n",
    "# Set the \"TORCH_HOME\" environment variable. Torch will download to this directory\n",
    "os.environ['TORCH_HOME'] = cache_dir"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to ./data/models/hub\\checkpoints\\fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0.00/167M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b6942eccfd246f7ade3eebf23b16464"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the most up-to-date weights for our FasterRCNN pretrained model\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "# Load our pretrained model using torchvision\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we run the above cell, we see our model is downloaded to our specified `cache_dir`. Each subsequent time we load our model, it will be loaded from our cached directory, so there will be no need to download it.\n",
    "\n",
    "Now let's take a peek at what we'll be using from our pretrained model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "RoIHeads(\n  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n  (box_head): FastRCNNConvFCHead(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (1): Conv2dNormActivation(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (2): Conv2dNormActivation(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (3): Conv2dNormActivation(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (4): Flatten(start_dim=1, end_dim=-1)\n    (5): Linear(in_features=12544, out_features=1024, bias=True)\n    (6): ReLU(inplace=True)\n  )\n  (box_predictor): FastRCNNPredictor(\n    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roi_heads"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Without going into explicit detail regarding the Faster-RCNN model, we will just inspect the output layer (roi_heads) that we are interested in. [The Faster-RCNN paper](https://arxiv.org/abs/1506.01497) can be consulted to know the exact inner workings of the model.\n",
    "\n",
    "We see above that our model's region of interest (roi) heads contain 3 different heads.\n",
    "\n",
    "The first head is a region of interest (roi) pooling layer. This layer is responsible for sampling various region proposals generated by the region proposal network (RPN) and selecting probable candidates that contain an object.\n",
    "\n",
    "The second head is a box head of type FastRCNNConvFCHead, a built in torchvision type. This head consists of several 2D convolutional activation layers and these layers are comprised of a 2D convolution layer, a batch norm layer, and a rectified linear unit (ReLU) layer. The final layers in our box_head layer are layers to \"flatten\" (shrink the dimensions) of the output, followed by a Linear layer with a ReLU unit applied to it. The Linear layer here is simply a fully connected layer.\n",
    "\n",
    "Our box_predictor head is a FastRCNNPredictor TorchVision type variable that is comprised of a `cls_score` layer and `bbox_pred` layer. The classification score (`cls_score`) layer is responsible for predicting a label for a predicted bounding box and our bounding box prediction (`bbox_pred`) layer is responsible for predicting the bounding box. Each of these layers are Linear layers.\n",
    "\n",
    "If we look closely at the final Linear layer of the `box_head`, we see that it generates output features of size 1024. If we look at the input features of both Linear layers in the `box_predictor` head, we see that it accepts 1024 features as input. The output dimension of our `box_predictor` head for the classification score and bounding box predictions are 91 and 364 respectively. There are 90 (+1 for the background class remember?) different objects in the COCO dataset that our TorchVision FasterRCNN model was trained on. There are 4 points per bounding box, hence our output of size 364 for the bounding box prediction head."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modifying Our Model To Work With Fruits Data\n",
    "The output size of 91 and 364 is all fine and well, but it cannot account for our fruits data. And since we are only classifying various fruits, we really don't care about the 90 non-background classes.\n",
    "\n",
    "Thus, we need to modify our model to account for fruits and only fruits data. In particular, we'll be adding our own FastRCNNPredictor as our `box_predictor` head and setting it up to account for our 3 different types of fruit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# We have to account for the background class, hence 4\n",
    "num_classes = 4\n",
    "# Grab the input features from the classification score layer\n",
    "input_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# Create a new FastRCNNPredictor box predictor to work with our classes\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(input_features, num_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "RoIHeads(\n  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n  (box_head): FastRCNNConvFCHead(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (1): Conv2dNormActivation(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (2): Conv2dNormActivation(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (3): Conv2dNormActivation(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (4): Flatten(start_dim=1, end_dim=-1)\n    (5): Linear(in_features=12544, out_features=1024, bias=True)\n    (6): ReLU(inplace=True)\n  )\n  (box_predictor): FastRCNNPredictor(\n    (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n    (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n  )\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roi_heads"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we inspect our updated model, we see that we did not change the pooling or bounding box head at all. That's because we want to use the pretrained weights for those tasks. However, we did add in our new box predictor head that takes in the 1024 input features and outputs only 4 values for the `cls_score` layer and 16 values for the `bbox_pred` layer. Our `cls_score` layer has 4 outputs because every class in our dataset is assigned a score. The class with the highest score for the predicted box will be chosen."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting Up Training\n",
    "The first thing we need to do is inspect the list of model parameters. These are the weights that we'll be training during our fine-tuning step.\n",
    "\n",
    "We will set up an optimizer and learning rate scheduler for updating our weights based on the losses computed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspecting model parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "209"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [p for p in model.parameters()]\n",
    "len(params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "176"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_update = [p for p in model.parameters() if p.requires_grad]\n",
    "len(params_update)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above we can see that there are 209 total parameters in our model that we can train on. Of these, only 176 require gradient updates during training. In our situation, we need to update the weights of all trained parameters as we are changing the output of the model to our fruit classes vs. the 90 total classes of the COCO datasets. In some instances, you will want to keep the weights for the entire layer and only train on added layers. For these cases you can freeze all original parameters of the model and only set the layers that you added to require gradient updates."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Define our AdamW optimizer with a higher learning rate to start\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.AdamW(params, lr=lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Define our learning rate scheduler that will decrease the lr as training runs\n",
    "step_size = 3  # Number of epochs to run before decaying the learning rate\n",
    "gamma = 0.1   # The multiplicative value to reduce the learning rate by. We'll use 10%\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have our parameters, optimizer, and learning rate scheduler set up, we are almost ready to write our training loop. Before we do that, let's run one sample through our model and determine what the outputs are. We will use our Fruits Dataset custom class for this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from fruits_dataset import FruitsDataset, fruits_collate_fn\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Let's create a dataset and dataloader from our training directory images/annotations\n",
    "training_img_dir = './data/fruits/train/'\n",
    "training_annotations_file = './data/fruits/train/annotations.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Create our dataset and dataloader\n",
    "train_dataset = FruitsDataset(training_annotations_file, training_img_dir)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=fruits_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only want to grab one image/target from our dataset for now that we can run through our model. Let's do that now"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "image, target = next(iter(train_dataloader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'loss_classifier': tensor(1.6941, grad_fn=<NllLossBackward0>),\n 'loss_box_reg': tensor(0.5060, grad_fn=<DivBackward0>),\n 'loss_objectness': tensor(0.0098, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n 'loss_rpn_box_reg': tensor(0.0054, grad_fn=<DivBackward0>)}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(image, target)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above we see that our loss function is Negative Log Likelihood loss (NllLossBackward), which is a standard loss function when training neural nets. What we want to do is use this loss function with our optimizer and learning rate scheduler to update the weights of our network so that it learns to predict each of our fruits.\n",
    "\n",
    "To do this, we need to define a training loop for our model that will run the entire dataset through the model once and then update the weight gradients. When discussing training, when the entire set is ran through the model, this is referred to as one epoch. Because we are working with large data, it is likely impossible to run the data through and update the gradients in one pass. As such, we usually batch our data into smaller pieces using the dataloader. When discussing training, a single batch ran through the model is referred to as one training step.\n",
    "\n",
    "If we have access to a GPU, model training will be more efficient. We'll begin by determining if we have GPU access and if so we will use it for training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code was modified from the torchvision code available [here](https://github.com/pytorch/vision/blob/e59cf64bb6eb4c3a50e0a76d8019fa4c4d5f2a15/references/detection/engine.py)\n",
    "\n",
    "To avoid cloning the entire repo, we just grab the bits and pieces we need and use them.\n",
    "\n",
    "Let's define a function that will create a model for us and then proceed with defining our training and evaluation loops. Our model definition function will mimic what we did in the cells above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_epoch(_model, _optimizer, _data_loader, _device, _epoch):\n",
    "    # Set our model to training mode so weights will be updated\n",
    "    _model.train()\n",
    "    _lr_scheduler = None\n",
    "    if _epoch == 0:\n",
    "        # On the first epoch create a linear lr scheduler that will jumpstart the convergence hopefully\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(_data_loader) - 1)\n",
    "        _lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "             _optimizer, start_factor=warmup_factor, total_iters=warmup_iters)\n",
    "\n",
    "    for images, targets in _data_loader:\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After we've completed a training epoch, we'll likely want to evaluate our model's performance. We will write an evaluation loop to accomplish this. Our evaluation set is a small set of samples withheld from the training data so that we can ensure the model is actually learning and not overfitting to the training data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
