{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine Tuning a PyTorch Model on Our Fruits Dataset\n",
    "This notebook will demonstrate how to use PyTorch to fine tune a pre-existing object detection model for our fruits data set.\n",
    "\n",
    "We will begin by creating our own DataSet class that our training module will use for data batching.\n",
    "\n",
    "Then we will load a pre-trained Faster RCNN model using the PyTorch library and modify it so that it can classify the various fruits in our data set.\n",
    "\n",
    "Afterwards, we will write our training, evaluation, and testing loops used for training our model.\n",
    "During training, we will set it up so that if training is interrupted it can be resumed from where it stopped.\n",
    "We will see how to save our \"best\" model found during our training loop.\n",
    "Once training is completed, we will use our test data set to evaluate the precision, recall, and accuracy of our model.\n",
    "\n",
    "Finally, we will discuss next steps, including making our model work with the PyTorch lightning framework."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building a Custom DataSet Class for Our Fruits Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch provides a Dataset class that we can work with for batching during model training.\n",
    "We will begin by creating our own custom Dataset class that inherits from the base PyTorch Dataset class.\n",
    "Once we have our custom Dataset class, we can use the PyTorch Dataloader module to index our data for training, evaluation, and testing.\n",
    "The official [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) contains detailed information and links to the official documentation for both the Dataset and Dataloader class.\n",
    "\n",
    "We will start by defining our Dataset class and then explain what each function does.\n",
    "Afterwards, we'll create some Dataloaders using our custom Dataset class and then load a few examples to verify our code is working correctly\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Custom Fruits Dataset Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# The dataset class requires us to implement at least the __getitem__ and __len__ functions\n",
    "class FruitsDataset(Dataset):\n",
    "    def __init__(self, annotations_file: str, img_dir: str, transform=None, target_transform=None):\n",
    "        self.data_df = pd.read_csv(annotations_file, encoding='utf-8', engine='python')\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = self.data_df['label']\n",
    "        self.img_ids = self.data_df['filename'].unique()  # Use the image file name as the ID for each image\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.img_ids.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_id = self.img_ids[index]\n",
    "\n",
    "        # The full file path to the image\n",
    "        img_path = self.img_dir + img_id\n",
    "\n",
    "        # Select all rows in our data frame that contain entries for the image at location data_df[index]\n",
    "        img_annotations = self.data_df.loc[self.data_df['filename'] == img_id]\n",
    "\n",
    "        # Get the boxes and labels for our image\n",
    "        # Convert them to torch tensors so we can use them in our dataloader\n",
    "        boxes = img_annotations[['x1', 'y1', 'x2', 'y2']].values\n",
    "        # Get the area of all the boxes. The [:, 3] notation says to give me the entire column at column index 3\n",
    "        # and so on. This is numpy shorthand for subtracting and multiplying entire columns of arrays\n",
    "        # The area equation is w*h, and we will have an n-element matrix where each entry is the area of\n",
    "        # the bounding box for the nth object\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.int)\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        labels = img_annotations['label'].values\n",
    "        labels = [self.__convert_labels__(x) for x in labels]\n",
    "        labels = torch.as_tensor(labels)\n",
    "\n",
    "        # Read the image using torchvision so we can return it\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        # Apply any transforms if they were supplied\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            labels = self.target_transform(labels)\n",
    "\n",
    "        # Add the results for our image to a dictionary. This dictionary will hold the\n",
    "        # box, labels, and area of our box\n",
    "        target = {'boxes': boxes,\n",
    "                  'area': area,\n",
    "                  'labels': labels,\n",
    "                  'img_id': img_id}\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __convert_labels__(self, x):\n",
    "        # This function will convert our string labels into a one hot encoding value\n",
    "        # Torch will now allow creating a tensor using strings so our workaround will be\n",
    "        # to use this encoding.\n",
    "        # Remember, torch reserves 0 for the \"background class\" so we start at 1\n",
    "        # TODO: This may not be true for Faster-RCNN, just for Mask-RCNN\n",
    "        encoding = {'apple': 1, 'banana': 2, 'orange': 3, 'mixed': 4}\n",
    "        converted_label = encoding[x]\n",
    "        return converted_label\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explanation of Our Custom FruitsDataset Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `__init__()` function\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `__len__()` function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `__getitem__()` function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `__convert_labels__()` function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
