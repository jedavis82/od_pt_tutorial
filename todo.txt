Write function to visualize the image and ground truth/predicted bounding boxes
    I need to verify that the scaling I do for the images/boxes preserves the scale and location of the boxes

For verification of the below 2 notes, create an object detection head like they do in the articles I have
pulled up. I'm almost positive these will only compute one bounding box per image, which is pretty trash
for a "train an object detector tutorial"

Read the SSD and YOLO articles in my email. I am going to have to create my own box regression head.
    I need to understand the architectures and how they are trained.
    SSD may be the way to go since YOLO seems to depend on darknet a lot.
    SSD by default uses VGG which could be replaced with ResNet.
    YOLO may be able to be replaced like this, but I need to read about YOLOv5-8

Look into vision transformers
    At this point it may be easier to define and train a small vision transformer.
    What I'm noticing here is that there is limited information on training CNNs for this.
    The information out there isn't accurate and if it is correct it doesn't account for multiple box regression.
    ViTs may do this easier. What I mean is for stuff like SSD they just propose a bunch of boxes and compute
    losses to predict the right ones. YOLO does a sliding window approach where boxes are predicted and losses
    are computed on those.

