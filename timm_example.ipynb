{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## TIMM Example\n",
    "Working through the [medium article](https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055).\n",
    "PyTorch Image Models (timm) contains several model implementations that can be downloaded and used off the shelf or trained. We're going to switch from using Torchvision to TIMM for ease of use. TIMM is also supported by PyTorch where it seems the Torchvision models repo is no longer being added to or maintained."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['vgg11',\n 'vgg11_bn',\n 'vgg13',\n 'vgg13_bn',\n 'vgg16',\n 'vgg16_bn',\n 'vgg19',\n 'vgg19_bn']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('vgg*', pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "['resnet10t',\n 'resnet14t',\n 'resnet18',\n 'resnet18d',\n 'resnet26',\n 'resnet26d',\n 'resnet26t',\n 'resnet32ts',\n 'resnet33ts',\n 'resnet34',\n 'resnet34d',\n 'resnet50',\n 'resnet50_gn',\n 'resnet50d',\n 'resnet51q',\n 'resnet61q',\n 'resnet101',\n 'resnet101d',\n 'resnet152',\n 'resnet152d',\n 'resnet200d',\n 'resnetaa50',\n 'resnetblur50',\n 'resnetrs50',\n 'resnetrs101',\n 'resnetrs152',\n 'resnetrs200',\n 'resnetrs270',\n 'resnetrs350',\n 'resnetrs420',\n 'resnetv2_50',\n 'resnetv2_50d_evos',\n 'resnetv2_50d_gn',\n 'resnetv2_50x1_bit_distilled',\n 'resnetv2_50x1_bitm',\n 'resnetv2_50x1_bitm_in21k',\n 'resnetv2_50x3_bitm',\n 'resnetv2_50x3_bitm_in21k',\n 'resnetv2_101',\n 'resnetv2_101x1_bitm',\n 'resnetv2_101x1_bitm_in21k',\n 'resnetv2_101x3_bitm',\n 'resnetv2_101x3_bitm_in21k',\n 'resnetv2_152x2_bit_teacher',\n 'resnetv2_152x2_bit_teacher_384',\n 'resnetv2_152x2_bitm',\n 'resnetv2_152x2_bitm_in21k',\n 'resnetv2_152x4_bitm',\n 'resnetv2_152x4_bitm_in21k']"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('resnet*', pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "data": {
      "text/plain": "['cs3darknet_focus_l',\n 'cs3darknet_focus_m',\n 'cs3darknet_l',\n 'cs3darknet_m',\n 'cs3darknet_x',\n 'cs3sedarknet_l',\n 'cs3sedarknet_x',\n 'cspdarknet53',\n 'darknet53',\n 'darknetaa53']"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('*dark*', pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Set the download location for the models\n",
    "cache_dir = './data/models/'\n",
    "os.environ['TORCH_HOME'] = cache_dir"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "res_model = timm.create_model('resnet50d', pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "inc_model = timm.create_model('inception_resnet_v2', pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num layers:  10\n",
      "Final 2 layers:  [SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1)), Linear(in_features=2048, out_features=1000, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the layers of the resnet model\n",
    "res_layers = list(res_model.children())\n",
    "print('Num layers: ', len(res_layers)) # How many layers? 10\n",
    "print('Final 2 layers: ', res_layers[8:]) # What's in the final 2 layers? The classifier head\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num layers:  17\n",
      "Final 2 layers:  [SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1)), Linear(in_features=1536, out_features=1000, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the layers of the inception model\n",
    "inc_layers = list(inc_model.children())\n",
    "print('Num layers: ', len(inc_layers)) # What's in the final 2 layers? 17\n",
    "print('Final 2 layers: ', inc_layers[15:]) # What's in the final 2 layers? The classifier head\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "{'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth',\n 'num_classes': 1000,\n 'input_size': (3, 224, 224),\n 'pool_size': (7, 7),\n 'crop_pct': 0.875,\n 'interpolation': 'bilinear',\n 'mean': (0.485, 0.456, 0.406),\n 'std': (0.229, 0.224, 0.225),\n 'first_conv': 'conv1',\n 'classifier': 'fc',\n 'architecture': 'resnet34'}"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_model.default_cfg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2048, out_features=1000, bias=True)\n",
      "Linear(in_features=2048, out_features=1000, bias=True)\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "# Can see from above that the classifier is \"fc\". Both of these calls return the same thing\n",
    "print(res_model.get_classifier())\n",
    "print(res_model.fc)\n",
    "print('Input features: ', res_model.get_classifier().in_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named classifier head:  classif\n",
      "{'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/inception_resnet_v2-940b1cd6.pth', 'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8), 'crop_pct': 0.8975, 'interpolation': 'bicubic', 'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5), 'first_conv': 'conv2d_1a.conv', 'classifier': 'classif', 'label_offset': 1, 'architecture': 'inception_resnet_v2'}\n"
     ]
    }
   ],
   "source": [
    "print('Named classifier head: ',inc_model.default_cfg['classifier'])  # To get the name of the classifier head\n",
    "print(inc_model.default_cfg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features:  1536\n",
      "Linear(in_features=1536, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Can see from above that the classifier is \"classif\". Both of these calls return the same thing\n",
    "print('Input features: ', inc_model.get_classifier().in_features)\n",
    "print(inc_model.classif)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspecting the models in a similar fashion to the article here:\n",
    "[Notebook](https://jovian.ml/aakanksha-ns/road-signs-bounding-box-prediction)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "[Sequential(\n   (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n   (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n   (2): ReLU(inplace=True)\n   (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n   (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n   (5): ReLU(inplace=True)\n   (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n ),\n BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n ReLU(inplace=True),\n MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n Sequential(\n   (0): Bottleneck(\n     (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act1): ReLU(inplace=True)\n     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (drop_block): Identity()\n     (act2): ReLU(inplace=True)\n     (aa): Identity()\n     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act3): ReLU(inplace=True)\n     (downsample): Sequential(\n       (0): Identity()\n       (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n       (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     )\n   )\n   (1): Bottleneck(\n     (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act1): ReLU(inplace=True)\n     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (drop_block): Identity()\n     (act2): ReLU(inplace=True)\n     (aa): Identity()\n     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act3): ReLU(inplace=True)\n   )\n   (2): Bottleneck(\n     (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act1): ReLU(inplace=True)\n     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (drop_block): Identity()\n     (act2): ReLU(inplace=True)\n     (aa): Identity()\n     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act3): ReLU(inplace=True)\n   )\n ),\n Sequential(\n   (0): Bottleneck(\n     (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act1): ReLU(inplace=True)\n     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (drop_block): Identity()\n     (act2): ReLU(inplace=True)\n     (aa): Identity()\n     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act3): ReLU(inplace=True)\n     (downsample): Sequential(\n       (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n       (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n       (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     )\n   )\n   (1): Bottleneck(\n     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act1): ReLU(inplace=True)\n     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (drop_block): Identity()\n     (act2): ReLU(inplace=True)\n     (aa): Identity()\n     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act3): ReLU(inplace=True)\n   )\n   (2): Bottleneck(\n     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act1): ReLU(inplace=True)\n     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (drop_block): Identity()\n     (act2): ReLU(inplace=True)\n     (aa): Identity()\n     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act3): ReLU(inplace=True)\n   )\n   (3): Bottleneck(\n     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act1): ReLU(inplace=True)\n     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (drop_block): Identity()\n     (act2): ReLU(inplace=True)\n     (aa): Identity()\n     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (act3): ReLU(inplace=True)\n   )\n )]"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res_layers[:8]  # Take all but the classifier head\n",
    "# What are in the final two layers before the classifier head? And why do they split them in that article?\n",
    "\n",
    "# Looks like it's used to upsample the features. Likely the final feature extraction layer\n",
    "# res_layers[6:8]\n",
    "# Looks like all previous upsampling/conv2d layers\n",
    "res_layers[:6]\n",
    "# Still not sure why these are split up in the above article"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "[Block8(\n   (branch0): BasicConv2d(\n     (conv): Conv2d(2080, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n     (relu): ReLU()\n   )\n   (branch1): Sequential(\n     (0): BasicConv2d(\n       (conv): Conv2d(2080, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n       (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n       (relu): ReLU()\n     )\n     (1): BasicConv2d(\n       (conv): Conv2d(192, 224, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n       (bn): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n       (relu): ReLU()\n     )\n     (2): BasicConv2d(\n       (conv): Conv2d(224, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n       (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n       (relu): ReLU()\n     )\n   )\n   (conv2d): Conv2d(448, 2080, kernel_size=(1, 1), stride=(1, 1))\n ),\n BasicConv2d(\n   (conv): Conv2d(2080, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n   (bn): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n   (relu): ReLU()\n )]"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inc_layers[:15]  # Take all but the classifier head\n",
    "# What are in the final 2 layers before the classifier head? Why are they split up in the article?\n",
    "\n",
    "# Similar to the above, looks like it's used to upsample features\n",
    "# inc_layers[13:15]\n",
    "\n",
    "# Similarly, all previous conv layers. Still no idea why the split though.\n",
    "# inc_layers[:13]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "Linear(in_features=2048, out_features=10, bias=True)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.create_model('resnet50d', pretrained=True, num_classes=10).get_classifier()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet50d', pretrained=True, num_classes=10, global_pool='catavgmax')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "4096"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features = model.get_classifier().in_features\n",
    "in_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Replace the final layer with a modified classification head\n",
    "from torch import nn\n",
    "model.fc = nn.Sequential(\n",
    "    nn.BatchNorm1d(in_features),\n",
    "    nn.Linear(in_features=in_features, out_features=512, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(in_features=512, out_features=10, bias=False)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 10])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model(torch.randn(1, 3, 224, 224)).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There's a lot more information in the medium article, mostly involving using the built in dataset stuff. I'm not writing all of that out here because we'll likely have to use something similar to the dataset class we've already created."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
